{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model testing\n",
    "OK, we have done carry forward control, we have done simple linear models, we have done ARIMA - last thing to try before going on a data hunt in true neural network based models. We could try some regression with vanilla dense layers or something, but I think we should cut to the chase and go straight to a GRU based network. Let's get a minimal example set up to get an idea of what we are working with and then set up a more rigorous hyperparameter optimization with crossvalidation/bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import shelve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Add notebook parent dir to path so we can import from functions/\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project config file\n",
    "import config as conf\n",
    "\n",
    "# Import notebooks specific helper functions\n",
    "import functions.notebook_helper_functions.notebook15 as funcs\n",
    "\n",
    "# Instantiate paths and model parameters\n",
    "paths = conf.DataFilePaths()\n",
    "params = conf.GRU_model_parameters()\n",
    "\n",
    "# Load data column index\n",
    "index = shelve.open(paths.PARSED_DATA_COLUMN_INDEX)\n",
    "\n",
    "# Set number of counties to include in our testing\n",
    "num_counties = 100\n",
    "\n",
    "# Set data block size \n",
    "block_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfips: 0\n",
      "first_day_of_month: 1\n",
      "microbusiness_density: 2\n",
      "active: 3\n",
      "microbusiness_density_change: 4\n",
      "microbusiness_density_change_change: 5\n"
     ]
    }
   ],
   "source": [
    "for column, num in index.items():\n",
    "    print(f'{column}: {num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with block size\n",
    "input_file = f'{paths.PARSED_DATA_PATH}/{params.input_file_root_name}{block_size}.npy'\n",
    "timepoints = np.load(input_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so here we go. Hardest part about working with this type of model is getting the data into the correct shape for input. Couple of considerations here:\n",
    "1. Do training/validation split where data is kept in sequential time order with older data being used for training and newer data being used for validation.\n",
    "2. Start with only one input feature - the microbusiness density (or detrended microbusiness density).\n",
    "3. Forecast one point into the future.\n",
    "4. Be ready to standardize/unstandardize data using statistics from the training set only.\n",
    "5. Input data is formatted as (batch, timesteps, feature). \n",
    "\n",
    "The last part is a little complicated to think about - we have > 3k timeseries with the same time axis, one for each county. We could treat this like 3k features, but I think the better idea is to think of it as one feature and 3k counties * 37 timepoints input datapoints. The trick is, how do we batch/make timesteps out of it? We don't want to present the model timeseries from different counties as if one comes after the next. I think the most obvious way to do this is use a stateless GRU layer and then present each county as a batch. Within that batch we then have the block from our sliding window data parse.\n",
    "\n",
    "Ok, I think that sounds like as good a place to start as any. Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (29, 3135, 9, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input data shape: {timepoints.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions here are:\n",
    "\n",
    "0. The timepoint block - the sized of this axis depends on the width of the block used to scan the data - smaller blocks give more timepoints with num_timepoint_blocks = total_timepoints - block_size + 1. This is also the axis we need to do our training validation split on. First part becomes training, last part becomes validation.\n",
    "1. The counties - each element here is a county, for the purposes of our first experiment with this we will treat each county as a batch.\n",
    "2. The the timepoints in the timepoint block (~row in pandas dataframe).\n",
    "3. The features (~column in pandas dataframe). To start with, we will work with one feature only - the microbusiness density.\n",
    "\n",
    "First up - training/validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (29, 3135, 9, 1)\n",
      "Split fraction: 0.7\n",
      "Split index: 20\n",
      "Training data shape: (20, 3135, 9, 1)\n",
      "Validation data shape: (8, 3135, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "# Choose split\n",
    "split_index = int(timepoints.shape[0] * params.training_split_fraction)\n",
    "\n",
    "# Before we split, choose just the data we want and drop everything else\n",
    "input_data = timepoints[:,:,:,[index[params.input_data_type]]]\n",
    "\n",
    "# Split data into training and validation sets using chosen\n",
    "# index. First portion becomes training, second portion\n",
    "# is validation\n",
    "training_data = input_data[0:split_index]\n",
    "validation_data = input_data[split_index:-1]\n",
    "\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "print(f'Split fraction: {params.training_split_fraction}')\n",
    "print(f'Split index: {split_index}')\n",
    "print(f'Training data shape: {training_data.shape}')\n",
    "print(f'Validation data shape: {validation_data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks good - let's try converting everything to a z-score, using mean and standard deviation from the training sample only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 3.77, standard deviation: 4.52\n",
      "Training data, new mean: -0.00, new standard deviation: 1.00\n",
      "Validation data, new mean: 0.03, new standard deviation: 1.31\n"
     ]
    }
   ],
   "source": [
    "# Get mean and standard deviation from training data\n",
    "training_mean = np.mean(training_data)\n",
    "training_deviation = np.std(training_data)\n",
    "\n",
    "print(f'Mean: {training_mean:.2f}, standard deviation: {training_deviation:.2f}')\n",
    "\n",
    "# Standardize the training and validation data\n",
    "training_data = (training_data - training_mean) / training_deviation\n",
    "validation_data = (validation_data - training_mean) / training_deviation\n",
    "\n",
    "print(f'Training data, new mean: {np.mean(training_data):.2f}, new standard deviation: {np.std(training_data):.2f}')\n",
    "print(f'Validation data, new mean: {np.mean(validation_data):.2f}, new standard deviation: {np.std(validation_data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training data shape: (3135, 20, 9, 1)\n",
      "New validation data shape: (3135, 8, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "# Swap first and second axis to make the data batch major, instead\n",
    "# of time major - this is the more common format for tensorflow and what\n",
    "# our GRU layer(s) will expect to see\n",
    "training_data = np.swapaxes(training_data, 1, 0)\n",
    "validation_data = np.swapaxes(validation_data, 1, 0)\n",
    "\n",
    "print(f'New training data shape: {training_data.shape}')\n",
    "print(f'New validation data shape: {validation_data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's build the model. Only additional thing to mention here is that for each time block in the counties, the first n datapoints are the time ordered input and the last one is the value we are trying to predict. With that in mind, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GRU(\n",
    "    units: int = 64,\n",
    "    learning_rate: float = 0.0002,\n",
    "    input_shape: list[int] = [8,1]\n",
    "):\n",
    "    '''Builds GRU based neural network for\n",
    "    microbusiness density regression'''\n",
    "\n",
    "    # Input layer\n",
    "    input = layers.Input(\n",
    "        name = 'Input',\n",
    "        shape = input_shape\n",
    "    )\n",
    "\n",
    "    # GRU layer\n",
    "    gru = layers.GRU(\n",
    "        units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        go_backwards=False,\n",
    "        stateful=False,\n",
    "        unroll=False,\n",
    "        time_major=False,\n",
    "        reset_after=True,\n",
    "        name='GRU'\n",
    "    )(input)\n",
    "\n",
    "    # output layer\n",
    "    output = layers.Dense(\n",
    "        name = 'Output',\n",
    "        units = 1,\n",
    "        activation = 'linear'\n",
    "    )(gru)\n",
    "\n",
    "    # Next, we will build the complete model and compile it.\n",
    "    model = keras.models.Model(\n",
    "        input, \n",
    "        output,\n",
    "        name = 'Simple_GRU_model'\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss = keras.losses.MeanSquaredError(name = 'MSE'), \n",
    "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "        metrics = [keras.metrics.MeanAbsoluteError(name = 'MAE')]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (8, 1)\n",
      "Target shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(data):\n",
    "    '''Generates pairs of X, Y (input, target) datapoints\n",
    "    from batch major data'''\n",
    "    \n",
    "    while True:\n",
    "        # Loop on counties\n",
    "        for i in range(data.shape[0]):\n",
    "            # Loop on timepoints\n",
    "            for j in range(data.shape[1]):\n",
    "                \n",
    "                X = data[i,j,:-1,:]\n",
    "                Y = data[i,j,-1:,:]\n",
    "\n",
    "                yield (X, Y)\n",
    "\n",
    "# Start generator and grab a sample data point\n",
    "training_data_generator = data_generator(training_data)\n",
    "input_sample = next(training_data_generator)\n",
    "\n",
    "# Check sample point's shape\n",
    "print(f'Input shape: {input_sample[0].shape}')\n",
    "print(f'Target shape: {input_sample[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_GRU_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 8, 1)]            0         \n",
      "                                                                 \n",
      " GRU (GRU)                   (None, 64)                12864     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,929\n",
      "Trainable params: 12,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_GRU(\n",
    "    units = 64,\n",
    "    learning_rate = 0.0002,\n",
    "    input_shape = [input_sample[0].shape[0],input_sample[0].shape[1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/lib/cuda/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3135/3135 [==============================] - 23s 7ms/step - loss: 0.0211 - MAE: 0.0780 - val_loss: 1.0110 - val_MAE: 0.2061\n",
      "Epoch 2/10\n",
      "3135/3135 [==============================] - 20s 6ms/step - loss: 0.4261 - MAE: 0.1627 - val_loss: 0.3193 - val_MAE: 0.2468\n",
      "Epoch 3/10\n",
      "3135/3135 [==============================] - 20s 7ms/step - loss: 0.1109 - MAE: 0.1004 - val_loss: 0.0193 - val_MAE: 0.0256\n",
      "Epoch 4/10\n",
      "3135/3135 [==============================] - 20s 6ms/step - loss: 0.2918 - MAE: 0.0999 - val_loss: 0.0047 - val_MAE: 0.0416\n",
      "Epoch 5/10\n",
      "3135/3135 [==============================] - 21s 7ms/step - loss: 0.0401 - MAE: 0.0453 - val_loss: 1.9811 - val_MAE: 0.1058\n",
      "Epoch 6/10\n",
      "1833/3135 [================>.............] - ETA: 5s - loss: 0.0142 - MAE: 0.0539"
     ]
    }
   ],
   "source": [
    "# Fire data generators for training and validation\n",
    "training_data_generator = data_generator(training_data)\n",
    "validation_data_generator = data_generator(validation_data)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_data_generator,\n",
    "    epochs = 10,\n",
    "    batch_size = training_data.shape[1],\n",
    "    steps_per_epoch = training_data.shape[0],\n",
    "    validation_data = validation_data_generator,\n",
    "    validation_batch_size = validation_data.shape[1],\n",
    "    validation_steps = validation_data.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and label the training and validation loss values\n",
    "plt.plot(list(range(len(history.history['loss']))), history.history['loss'], label='Training Loss')\n",
    "plt.plot(list(range(len(history.history['val_loss']))), history.history['val_loss'], label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89e0f329143aafc2740b6540b46c06e92791a1e818eb6a9ece1d952786ba476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
