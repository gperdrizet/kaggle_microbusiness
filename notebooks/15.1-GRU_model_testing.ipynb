{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model testing\n",
    "OK, we have done carry forward control, we have done simple linear models, we have done ARIMA - last thing to try before going on a data hunt in true neural network based models. We could try some regression with vanilla dense layers or something, but I think we should cut to the chase and go straight to a GRU based network. Let's get a minimal example set up to get an idea of what we are working with and then set up a more rigorous hyperparameter optimization with crossvalidation/bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add notebook parent dir to path so we can import from functions/\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import shelve\n",
    "import numpy as np\n",
    "\n",
    "# Import project config file\n",
    "import config as conf\n",
    "\n",
    "# Import notebooks specific helper functions\n",
    "import functions.notebook_helper_functions.notebook15 as funcs\n",
    "\n",
    "# Instantiate paths and model parameters\n",
    "paths = conf.DataFilePaths()\n",
    "params = conf.GRU_model_parameters()\n",
    "\n",
    "# Load data column index\n",
    "index = shelve.open(paths.PARSED_DATA_COLUMN_INDEX)\n",
    "\n",
    "# Set number of counties to include in our testing\n",
    "num_counties = 100\n",
    "\n",
    "# Set data block size \n",
    "block_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfips: 0\n",
      "first_day_of_month: 1\n",
      "microbusiness_density: 2\n",
      "active: 3\n",
      "microbusiness_density_change: 4\n",
      "microbusiness_density_change_change: 5\n"
     ]
    }
   ],
   "source": [
    "for column, num in index.items():\n",
    "    print(f'{column}: {num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with block size\n",
    "input_file = f'{paths.PARSED_DATA_PATH}/{params.input_file_root_name}{block_size}.npy'\n",
    "timepoints = np.load(input_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so here we go. Hardest part about working with this type of model is getting the data into the correct shape for input. Couple of considerations here:\n",
    "1. Do training/validation split where data is kept in sequential time order with older data being used for training and newer data being used for validation.\n",
    "2. Start with only one input feature - the microbusiness density (or detrended microbusiness density).\n",
    "3. Forecast one point into the future.\n",
    "4. Be ready to standardize/unstandardize data using statistics from the training set only.\n",
    "5. Input data is formatted as (batch, timesteps, feature). \n",
    "\n",
    "The last part is a little complicated to think about - we have > 3k timeseries with the same time axis, one for each county. We could treat this like 3k features, but I think the better idea is to think of it as one feature and 3k counties * 37 timepoints input datapoints. The trick is, how do we batch/make timesteps out of it? We don't want to present the model timeseries from different counties as if one comes after the next. I think the most obvious way to do this is use a stateless GRU layer and then present each county as a batch. Within that batch we then have the block from our sliding window data parse.\n",
    "\n",
    "Ok, I think that sounds like as good a place to start as any. Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (29, 3135, 9, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input data shape: {timepoints.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions here are:\n",
    "\n",
    "0. The timepoint block - the sized of this axis depends on the width of the block used to scan the data - smaller blocks give more timepoints with num_timepoint_blocks = total_timepoints - block_size + 1. This is also the axis we need to do our training validation split on. First part becomes training, last part becomes validation.\n",
    "1. The counties - each element here is a county, for the purposes of our first experiment with this we will treat each county as a batch.\n",
    "2. The the timepoints in the timepoint block (~row in pandas dataframe).\n",
    "3. The features (~column in pandas dataframe).\n",
    "\n",
    "First up - training/validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (29, 3135, 9)\n",
      "Split fraction: 0.7\n",
      "Split index: 20\n",
      "Training data shape: (20, 3135, 9)\n",
      "Validation data shape: (8, 3135, 9)\n"
     ]
    }
   ],
   "source": [
    "# Choose split\n",
    "split_index = int(timepoints.shape[0] * params.training_split_fraction)\n",
    "\n",
    "# Before we split, choose just the data we want and drop everything else\n",
    "input_data = timepoints[:,:,:,index[params.input_data_type]]\n",
    "\n",
    "training_data = input_data[0:split_index]\n",
    "validation_data = input_data[split_index:-1]\n",
    "\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "print(f'Split fraction: {params.training_split_fraction}')\n",
    "print(f'Split index: {split_index}')\n",
    "print(f'Training data shape: {training_data.shape}')\n",
    "print(f'Validation data shape: {validation_data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks good - let's try converting everything to a z-score, using mean and standard deviation from the training sample only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 3.77, standard deviation: 4.52\n",
      "New mean: -0.00, New standard deviation: 1.00\n"
     ]
    }
   ],
   "source": [
    "training_mean = np.mean(training_data)\n",
    "training_deviation = np.std(training_data)\n",
    "\n",
    "print(f'Mean: {training_mean:.2f}, standard deviation: {training_deviation:.2f}')\n",
    "\n",
    "training_data = (training_data - training_mean) / training_deviation\n",
    "\n",
    "print(f'New mean: {np.mean(training_data):.2f}, New standard deviation: {np.std(training_data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape: (3135, 20, 9)\n"
     ]
    }
   ],
   "source": [
    "training_data = np.swapaxes(training_data, 1, 0)\n",
    "print(f'New shape: {training_data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's build the model. Only additional thing to mention here is that for each time block in the counties, the first n datapoints are the time ordered input and the last one is the value we are trying to predict. With that in mind, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GRU(\n",
    "    units: int = 64,\n",
    "    learning_rate: float = 0.002\n",
    "):\n",
    "\n",
    "    # Input layer\n",
    "    input = layers.Input(\n",
    "        name = 'Input',\n",
    "        shape = (20,8)\n",
    "    )\n",
    "\n",
    "    # GRU layer\n",
    "    gru = layers.GRU(\n",
    "        units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        go_backwards=False,\n",
    "        stateful=False,\n",
    "        unroll=False,\n",
    "        time_major=False,\n",
    "        reset_after=True,\n",
    "        name='GRU'\n",
    "    )(input)\n",
    "\n",
    "    # output layer\n",
    "    output = layers.Dense(\n",
    "        name = 'Output',\n",
    "        units = 1,\n",
    "        activation = 'linear'\n",
    "    )(gru)\n",
    "\n",
    "    # Next, we will build the complete model and compile it.\n",
    "    model = keras.models.Model(\n",
    "        input, \n",
    "        output,\n",
    "        name = 'Simple_GRU_model'\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss = keras.losses.MeanSquaredError(name = 'MSE'), \n",
    "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "        metrics = [keras.metrics.MeanAbsoluteError(name = 'MAE')]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data):\n",
    "    \n",
    "    while True:\n",
    "        for i in range(data.shape[1]):\n",
    "            X = data[:,i,:-1]\n",
    "            Y = data[:,i,-1:]\n",
    "\n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3135, 8)\n",
      "Target shape: (3135, 1)\n"
     ]
    }
   ],
   "source": [
    "training_data_generator = data_generator(training_data)\n",
    "input_sample = next(training_data_generator)\n",
    "\n",
    "print(f'Input shape: {input_sample[0].shape}')\n",
    "print(f'Target shape: {input_sample[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_GRU_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 20, 8)]           0         \n",
      "                                                                 \n",
      " GRU (GRU)                   (None, 64)                14208     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,273\n",
      "Trainable params: 14,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_GRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_1' defined at (most recent call last):\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_59692/1745903755.py\", line 4, in <module>\n      history = model.fit(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_1'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_1}}]] [Op:__inference_train_function_6875]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the model to the training data.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_data_generator \u001b[39m=\u001b[39m data_generator(training_data)\n\u001b[0;32m----> 4\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      5\u001b[0m     x\u001b[39m=\u001b[39;49mtraining_data[:,:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m     y\u001b[39m=\u001b[39;49mtraining_data[:,:,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m:],\n\u001b[1;32m      7\u001b[0m     \u001b[39m#batch_size = 20,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m#steps_per_epoch = steps_per_epoch,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m#validation_data = validation_data,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#validation_steps = validation_steps,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m#callbacks = callbacks\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/microbusiness/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_1' defined at (most recent call last):\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_59692/1745903755.py\", line 4, in <module>\n      history = model.fit(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/siderealyear/anaconda3/envs/microbusiness/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_1'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_1}}]] [Op:__inference_train_function_6875]"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data.\n",
    "training_data_generator = data_generator(training_data)\n",
    "\n",
    "history = model.fit(\n",
    "    x=training_data[:,:,:-1],\n",
    "    y=training_data[:,:,-1:],\n",
    "    #batch_size = 20,\n",
    "    epochs = 10,\n",
    "    #steps_per_epoch = steps_per_epoch,\n",
    "    #validation_data = validation_data,\n",
    "    #validation_steps = validation_steps,\n",
    "    verbose = True\n",
    "    #callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89e0f329143aafc2740b6540b46c06e92791a1e818eb6a9ece1d952786ba476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
