{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 5: Structured bootstrapping\n",
    "OK, so internal benchmarking strategy #2 here. Going to start calling it bootstrapping because I am using random sampling with replacement and am envisioning a bagging approach to the final model. Big takeaway from the first cross validation-like approach (really ended up being more like bootstrapping anyway...) was that to get good approximations of the public leader board score we should not be aggregating SMAPE scores across forecast origins. This seems to result in worse performance, the explanation being that we are including data from some anomalous and some nonanomalous timepoints. When, in reality a given timepoint is either an anomaly or not. This causes the naive model to do somewhat badly all the time rather than OK on nonanomalous test time points and very badly on anomalous ones.\n",
    "\n",
    "So, the strategy here will be to structure our bootstrapping while still treating each county as an individual datapoint. To generate a sample of size n the procedure will be as follows:\n",
    "1. Remove and sequester some timepoints dataset-wide for test data.\n",
    "2. From the remaining, pick a random timepoint.\n",
    "3. From the randomly chosen timepoint randomly choose n counties.\n",
    "4. Go to step 2.\n",
    "\n",
    "This procedure could be repeated with different test-train splits, or even test train splits on non-overlapping subsets of the data. Does that make it bootstrapping inside of cross-validation fold? I don't know - I think the specific terminology here is less important than a precise description of what we are actually doing.\n",
    "\n",
    "\n",
    "1. [Abbreviations & definitions](#abbrevations_definitions)\n",
    "2. [Load & inspect](#load_inspect)\n",
    "3. [Build data structure](#build_data_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"abbreviations_definitions\"></a>\n",
    "### 1. Abbreviations & definitions\n",
    "+ MBD: microbusiness density\n",
    "+ MBC: microbusiness count\n",
    "+ OLS: ordinary least squares\n",
    "+ Model order: number of past timepoints used as input data for model training\n",
    "+ Origin (forecast origin): last known point in the input data\n",
    "+ Horizon (forecast horizon): number of future data points predicted by the model\n",
    "+ SMAPE: Symmetric mean absolute percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_inspect\"></a>\n",
    "### 2. Load & inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to allow import of config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import config as conf\n",
    "import functions.data_manipulation_functions as data_funcs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from statistics import NormalDist\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print()\n",
    "print(f'Numpy {np.__version__}')\n",
    "print(f'Pandas {pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up training file, set dtype for first day of month\n",
    "paths = conf.DataFilePaths()\n",
    "\n",
    "training_df = pd.read_csv(f'{paths.KAGGLE_DATA_PATH}/train.csv.zip', compression='zip')\n",
    "training_df['first_day_of_month'] =  pd.to_datetime(training_df['first_day_of_month']).astype(int)\n",
    "training_df.drop(['row_id', 'county','state'], axis=1, inplace=True)\n",
    "training_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"build_data_structure\"></a>\n",
    "### 1. Build data structure\n",
    "First thing to do is build a data structure that will make resampling as easy as possible and persist it to disk. Then we will implement resampling. This way when experimenting with/training models, it will be easy to generate samples on they fly without having to rebuild the whole thing in memory each time.\n",
    "\n",
    "The trick here is that in the first dimension we want the timepoints, then the counties, then the data columns. But, the 'timepoints' we want in the first dimension are really the forecast origins for a subset of the timecourse.\n",
    "\n",
    "Maybe the best way to think about it is to forget the model order, forecast horizon and forecast origin location for now and just pick a data instance size. Then, when feeding the sample to a model we can break each block into input and forecast halves on the fly however we want. This will also solve the problem we had in notebook #07 where if model order != forecast horizon NumPy complains about ragged arrays.\n",
    "\n",
    "Not sure if there maybe would be a speed advantage of sharding the data to multiple files here - i.e. each timepoint gets its own file and then worker processes can be assigned to sample from and train on the timepoints independently. Let's save that for a later optimization to minimize complexity out of the gate and not spend time prematurely optimizing something that ends up not being the right idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing, let's give each timepoint an integer number so we don't have to \n",
    "# work with the strings/datetimes in 'first_day_of_month' directly and clean up\n",
    "# columns we won't need.\n",
    "\n",
    "training_df['timepoint'] = training_df.groupby(['cfips']).cumcount()\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add a column with difference detrended data (see notebook #02.2)\n",
    "\n",
    "# Makes sure the rows are in chronological order within each county\n",
    "training_df = training_df.sort_values(by=['cfips', 'first_day_of_month'])\n",
    "\n",
    "# Calculate and add column for month to month change in MBD\n",
    "training_df['microbusiness_density_change'] = training_df.groupby(['cfips'])['microbusiness_density'].diff() # type: ignore\n",
    "training_df.dropna(inplace=True)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to need a list of unique cfips IDs to retrieve the counties\n",
    "cfips_list = training_df['cfips'].drop_duplicates(keep='first').to_list()\n",
    "print(f'Num counties: {len(cfips_list)}')\n",
    "\n",
    "# The possible block left edges are the timepoints so get those in a list too\n",
    "num_timepoints = training_df['timepoint'].nunique()\n",
    "print(f'Num timepoints: {num_timepoints}\\n')\n",
    "\n",
    "for block_size in range(2,39):\n",
    "\n",
    "    timepoints = []\n",
    "\n",
    "    for left_edge in range(1, (num_timepoints - block_size + 1)):\n",
    "\n",
    "        # The right edge of this block is the left edge number\n",
    "        # plus the blocksize\n",
    "        right_edge = left_edge + block_size\n",
    "        print(f'\\tBlock range: {left_edge} - {right_edge}')\n",
    "\n",
    "        # Holder for individual blocks\n",
    "        blocks = []\n",
    "\n",
    "        # Now we go through each county and get this block range\n",
    "        for cfips in cfips_list:\n",
    "\n",
    "            # Get data for just this county\n",
    "            county_data = training_df[training_df['cfips'] == cfips]\n",
    "\n",
    "            # Get rows for the block range\n",
    "            county_data = county_data.loc[(county_data['timepoint'] >= left_edge) & (county_data['timepoint'] <= right_edge)]\n",
    "\n",
    "            # Convert block range rows to numpy and collect\n",
    "            blocks.append(county_data.to_numpy())\n",
    "        \n",
    "        # Convert list of blocks to numpy and collect\n",
    "        print(f'\\tBlock shape: {np.array(blocks).shape}\\n')\n",
    "\n",
    "        timepoints.append(np.array(blocks))\n",
    "\n",
    "    # Convert final result to numpy\n",
    "    timepoints = np.array(timepoints)\n",
    "\n",
    "    # Write to disk\n",
    "    output_file = f'{paths.DATA_PATH}/parsed_data/structured_bootstrap_blocksize{block_size}.npy'\n",
    "    np.save(output_file, timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Timepoints shape: {timepoints.shape}') # type: ignore\n",
    "print()\n",
    "print('Column types:')\n",
    "\n",
    "for column in timepoints[0,0,0,0:]: # type: ignore\n",
    "    print(f'\\t{type(column)}')\n",
    "\n",
    "print()\n",
    "print(f'Example block:\\n{timepoints[0,0,0:,]}') # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks good. We could use int for some of these columns, but we need float for the MBD, so let's leave it alone rather than using mixed types in a NumPy array. So, that's it - easy. Let's round trip it and try recovering some our values back into a nice pandas dataframe as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check round-trip\n",
    "loaded_timepoints = np.load(output_file)\n",
    "\n",
    "# Inspect\n",
    "print(f'Timepoints shape: {loaded_timepoints.shape}')\n",
    "print(f'Example block:\\n{loaded_timepoints[0,0,0:,]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, not surprisingly - we got the same thing back. Last thing to do before we call this done is to test if we can get our dates, row_ids and cfips back into a format that matches the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab an example date column\n",
    "test_dates = loaded_timepoints[0,0,0:,1] # type: ignore\n",
    "print(f'Test dates: {test_dates}\\ndtype: {type(test_dates)}\\nelement dtype: {type(test_dates[0])}\\n')\n",
    "\n",
    "# Cast float64 to int64\n",
    "test_dates = test_dates.astype(np.int64)\n",
    "print(f'Test dates: {test_dates}\\ndtype: {type(test_dates)}\\nelement dtype: {type(test_dates[0])}\\n')\n",
    "\n",
    "# Convert to pandas dataframe with dtype datetime64[ns] and column name 'first_day_of_month'\n",
    "test_dates_df = pd.DataFrame(pd.to_datetime(test_dates), columns=['first_day_of_month']).astype('datetime64')\n",
    "test_dates_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab an example cfips column\n",
    "test_cfips = loaded_timepoints[0,0,0:,0] # type: ignore\n",
    "print(f'Test cfips: {test_cfips}\\ndtype: {type(test_cfips)}\\nelement dtype: {type(test_cfips[0])}\\n')\n",
    "\n",
    "# Cast float64 to int64\n",
    "test_cfips = test_cfips.astype(np.int64)\n",
    "print(f'Test cfips: {test_cfips}\\ndtype: {type(test_cfips)}\\nelement dtype: {type(test_cfips[0])}\\n')\n",
    "\n",
    "# Convert to pandas dataframe with dtype int64 and column name 'cfips'\n",
    "test_cfips_df = pd.DataFrame(test_cfips, columns=['cfips']).astype('int64')\n",
    "test_cfips_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfips_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, happy - making the rwo ID is just a string join from here, so no problem. An if for some reason we want the string county or state back, we can use a CFIPS lookup table. Time to move on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89e0f329143aafc2740b6540b46c06e92791a1e818eb6a9ece1d952786ba476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
