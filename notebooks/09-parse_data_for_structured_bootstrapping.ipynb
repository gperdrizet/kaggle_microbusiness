{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 5: Structured bootstrapping\n",
    "OK, so internal benchmarking strategy #2 here. Going to start calling it bootstrapping because I am using random sampling with replacement and am envisioning a bagging approach to the final model. Big takeaway from the first cross validation-like approach (really ended up being more like bootstrapping anyway...) was that to get good approximations of the public leader board score we should not be aggregating SMAPE scores across forecast origins. This seems to result in worse performance, the explanation being that we are including data from some anomalous and some nonanomalous timepoints. When, in reality a given timepoint is either an anomaly or not. This causes the naive model to do somewhat badly all the time rather than OK on nonanomalous test time points and very badly on anomalous ones.\n",
    "\n",
    "So, the strategy here will be to structure our bootstrapping while still treating each county as an individual datapoint. To generate a sample of size n the procedure will be as follows:\n",
    "1. Remove and sequester some timepoints dataset-wide for test data.\n",
    "2. From the remaining, pick a random timepoint.\n",
    "3. From the randomly chosen timepoint randomly choose n counties.\n",
    "4. Go to step 2.\n",
    "\n",
    "This procedure could be repeated with different test-train splits, or even test train splits on non-overlapping subsets of the data. Does that make it bootstrapping inside of cross-validation fold? I don't know - I think the specific terminology here is less important than a precise description of what we are actually doing.\n",
    "\n",
    "\n",
    "1. [Abbreviations & definitions](#abbrevations_definitions)\n",
    "2. [Load & inspect](#load_inspect)\n",
    "3. [Build data structure](#build_data_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"abbreviations_definitions\"></a>\n",
    "### 1. Abbreviations & definitions\n",
    "+ MBD: microbusiness density\n",
    "+ MBC: microbusiness count\n",
    "+ OLS: ordinary least squares\n",
    "+ Model order: number of past timepoints used as input data for model training\n",
    "+ Origin (forecast origin): last known point in the input data\n",
    "+ Horizon (forecast horizon): number of future data points predicted by the model\n",
    "+ SMAPE: Symmetric mean absolute percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_inspect\"></a>\n",
    "### 2. Load & inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:24:10) [GCC 9.4.0]\n",
      "\n",
      "Numpy 1.23.5\n",
      "Pandas 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path to allow import of config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import config as conf\n",
    "#import functions.data_manipulation_functions as data_funcs\n",
    "\n",
    "import shelve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import multiprocessing as mp\n",
    "#from statistics import NormalDist\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print()\n",
    "print(f'Numpy {np.__version__}')\n",
    "print(f'Pandas {pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfips</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>active</th>\n",
       "      <th>microbusiness_density_change</th>\n",
       "      <th>microbusiness_density_change_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1569888000000000000</td>\n",
       "      <td>3.055843</td>\n",
       "      <td>1269</td>\n",
       "      <td>0.170973</td>\n",
       "      <td>0.293785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1572566400000000000</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1243</td>\n",
       "      <td>-0.062610</td>\n",
       "      <td>-0.233583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1575158400000000000</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001</td>\n",
       "      <td>1577836800000000000</td>\n",
       "      <td>2.969090</td>\n",
       "      <td>1242</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>-0.024143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001</td>\n",
       "      <td>1580515200000000000</td>\n",
       "      <td>2.909326</td>\n",
       "      <td>1217</td>\n",
       "      <td>-0.059764</td>\n",
       "      <td>-0.035621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cfips   first_day_of_month  microbusiness_density  active  \\\n",
       "2   1001  1569888000000000000               3.055843    1269   \n",
       "3   1001  1572566400000000000               2.993233    1243   \n",
       "4   1001  1575158400000000000               2.993233    1243   \n",
       "5   1001  1577836800000000000               2.969090    1242   \n",
       "6   1001  1580515200000000000               2.909326    1217   \n",
       "\n",
       "   microbusiness_density_change  microbusiness_density_change_change  \n",
       "2                      0.170973                             0.293785  \n",
       "3                     -0.062610                            -0.233583  \n",
       "4                      0.000000                             0.062610  \n",
       "5                     -0.024143                            -0.024143  \n",
       "6                     -0.059764                            -0.035621  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up training file, set dtype for first day of month\n",
    "paths = conf.DataFilePaths()\n",
    "\n",
    "training_df = pd.read_csv(f'{paths.KAGGLE_DATA_PATH}/train.csv.zip', compression='zip')\n",
    "\n",
    "# Convert timepoint to datetime int because we are going to use numpy for the final data structure\n",
    "training_df['first_day_of_month'] =  pd.to_datetime(training_df['first_day_of_month']).astype(int)\n",
    "\n",
    "# Prune redundant columns\n",
    "training_df.drop(['row_id', 'county','state'], axis=1, inplace=True)\n",
    "\n",
    "# Let's also add a column with difference detrended data (see notebook #02.2)\n",
    "\n",
    "# Makes sure the rows are in chronological order within each county\n",
    "training_df = training_df.sort_values(by=['cfips', 'first_day_of_month'])\n",
    "\n",
    "# Calculate and add column for month to month change in MBD (first order difference)\n",
    "training_df['microbusiness_density_change'] = training_df.groupby(['cfips'])['microbusiness_density'].diff() # type: ignore\n",
    "\n",
    "# Calculate and add column for month to month change in MBD change (second order difference)\n",
    "training_df['microbusiness_density_change_change'] = training_df.groupby(['cfips'])['microbusiness_density_change'].diff() # type: ignore\n",
    "\n",
    "# Now the first two rows of each counties time series has a NAN, because there is no preceding timepoint to subtract.\n",
    "# So just drop those rows resulting in a total timeseries size of 37\n",
    "training_df.dropna(inplace=True)\n",
    "\n",
    "# Inspect\n",
    "training_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"build_data_structure\"></a>\n",
    "### 1. Build data structure\n",
    "First thing to do is build a data structure that will make resampling as easy as possible and persist it to disk. Then we will implement resampling. This way when experimenting with/training models, it will be easy to generate samples on they fly without having to rebuild the whole thing in memory each time.\n",
    "\n",
    "The trick here is that in the first dimension we want the timepoints, then the counties, then the data columns. But, the 'timepoints' we want in the first dimension are really the forecast origins for a subset of the timecourse.\n",
    "\n",
    "Maybe the best way to think about it is to forget the model order, forecast horizon and forecast origin location for now and just pick a data instance size. Then, when feeding the sample to a model we can break each block into input and forecast halves on the fly however we want. This will also solve the problem we had in notebook #07 where if model order != forecast horizon NumPy complains about ragged arrays.\n",
    "\n",
    "Not sure if there maybe would be a speed advantage of sharding the data to multiple files here - i.e. each timepoint gets its own file and then worker processes can be assigned to sample from and train on the timepoints independently. It also might not be a bad idea to us h5py. Let's save that for a later optimization to minimize complexity out of the gate and not spend time prematurely optimizing something that ends up not being the right idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num counties: 3135\n",
      "Num timepoints: 37\n",
      "Timepoints shape: (36, 3135, 2, 6)\n",
      "Timepoints shape: (35, 3135, 3, 6)\n",
      "Timepoints shape: (34, 3135, 4, 6)\n",
      "Timepoints shape: (33, 3135, 5, 6)\n",
      "Timepoints shape: (32, 3135, 6, 6)\n",
      "Timepoints shape: (31, 3135, 7, 6)\n",
      "Timepoints shape: (30, 3135, 8, 6)\n",
      "Timepoints shape: (29, 3135, 9, 6)\n",
      "Timepoints shape: (28, 3135, 10, 6)\n",
      "Timepoints shape: (27, 3135, 11, 6)\n",
      "Timepoints shape: (26, 3135, 12, 6)\n",
      "Timepoints shape: (25, 3135, 13, 6)\n",
      "Timepoints shape: (24, 3135, 14, 6)\n",
      "Timepoints shape: (23, 3135, 15, 6)\n",
      "Timepoints shape: (22, 3135, 16, 6)\n",
      "Timepoints shape: (21, 3135, 17, 6)\n",
      "Timepoints shape: (20, 3135, 18, 6)\n",
      "Timepoints shape: (19, 3135, 19, 6)\n",
      "Timepoints shape: (18, 3135, 20, 6)\n",
      "Timepoints shape: (17, 3135, 21, 6)\n",
      "Timepoints shape: (16, 3135, 22, 6)\n",
      "Timepoints shape: (15, 3135, 23, 6)\n",
      "Timepoints shape: (14, 3135, 24, 6)\n",
      "Timepoints shape: (13, 3135, 25, 6)\n",
      "Timepoints shape: (12, 3135, 26, 6)\n",
      "Timepoints shape: (11, 3135, 27, 6)\n",
      "Timepoints shape: (10, 3135, 28, 6)\n",
      "Timepoints shape: (9, 3135, 29, 6)\n",
      "Timepoints shape: (8, 3135, 30, 6)\n",
      "Timepoints shape: (7, 3135, 31, 6)\n",
      "Timepoints shape: (6, 3135, 32, 6)\n",
      "Timepoints shape: (5, 3135, 33, 6)\n",
      "Timepoints shape: (4, 3135, 34, 6)\n",
      "Timepoints shape: (3, 3135, 35, 6)\n",
      "Timepoints shape: (2, 3135, 36, 6)\n",
      "Timepoints shape: (1, 3135, 37, 6)\n"
     ]
    }
   ],
   "source": [
    "# Going to need a list of unique cfips IDs to retrieve the counties\n",
    "cfips_list = training_df['cfips'].drop_duplicates(keep='first').to_list()\n",
    "print(f'Num counties: {len(cfips_list)}')\n",
    "\n",
    "# Get number of timepoints per county\n",
    "num_timepoints = training_df['first_day_of_month'].nunique()\n",
    "print(f'Num timepoints: {num_timepoints}')\n",
    "\n",
    "# Start loop with three points - two for input and one for forecast\n",
    "# this is the smallest possible block for any sort of model that\n",
    "# does more than carry forward the last datapoint\n",
    "for block_size in range(2, num_timepoints + 1):\n",
    "\n",
    "    timepoints = []\n",
    "\n",
    "    for left_edge in range(0, (num_timepoints - block_size + 1)):\n",
    "\n",
    "        # Holder for individual blocks\n",
    "        blocks = []\n",
    "\n",
    "        # Now we go through each county and get this block range\n",
    "        for cfips in cfips_list:\n",
    "\n",
    "            # Get data for just this county\n",
    "            county_data = training_df[training_df['cfips'] == cfips]\n",
    "\n",
    "            # Get rows for the block range\n",
    "            county_data = county_data.iloc[left_edge:(left_edge + block_size)]\n",
    "\n",
    "            # Convert block range rows to numpy and collect\n",
    "            blocks.append(county_data)\n",
    "        \n",
    "        # Convert list of blocks to numpy and collect\n",
    "        timepoints.append(np.array(blocks))\n",
    "\n",
    "    # Convert final result to numpy\n",
    "    timepoints = np.array(timepoints)\n",
    "    print(f'Timepoints shape: {timepoints.shape}')\n",
    "\n",
    "    # Write to disk\n",
    "    output_file = f'{paths.DATA_PATH}/parsed_data/structured_bootstrap_blocksize{block_size}.npy'\n",
    "    np.save(output_file, timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index for column names\n",
    "\n",
    "with shelve.open(paths.PARSED_DATA_COLUMN_INDEX, 'c') as col_index:\n",
    "    for i, col_name in enumerate(county_data.columns):\n",
    "        col_index[col_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timepoints shape: (1, 3135, 37, 6)\n",
      "\n",
      "Column types:\n",
      "\t<class 'numpy.float64'>\n",
      "\t<class 'numpy.float64'>\n",
      "\t<class 'numpy.float64'>\n",
      "\t<class 'numpy.float64'>\n",
      "\t<class 'numpy.float64'>\n",
      "\t<class 'numpy.float64'>\n",
      "\n",
      "Example block:\n",
      "[[ 1.0010000e+03  1.5698880e+18  3.0558431e+00  1.2690000e+03\n",
      "   1.7097300e-01  2.9378470e-01]\n",
      " [ 1.0010000e+03  1.5725664e+18  2.9932332e+00  1.2430000e+03\n",
      "  -6.2609900e-02 -2.3358290e-01]\n",
      " [ 1.0010000e+03  1.5751584e+18  2.9932332e+00  1.2430000e+03\n",
      "   0.0000000e+00  6.2609900e-02]\n",
      " [ 1.0010000e+03  1.5778368e+18  2.9690900e+00  1.2420000e+03\n",
      "  -2.4143200e-02 -2.4143200e-02]\n",
      " [ 1.0010000e+03  1.5805152e+18  2.9093256e+00  1.2170000e+03\n",
      "  -5.9764400e-02 -3.5621200e-02]\n",
      " [ 1.0010000e+03  1.5830208e+18  2.9332314e+00  1.2270000e+03\n",
      "   2.3905800e-02  8.3670200e-02]\n",
      " [ 1.0010000e+03  1.5856992e+18  3.0001674e+00  1.2550000e+03\n",
      "   6.6936000e-02  4.3030200e-02]\n",
      " [ 1.0010000e+03  1.5882912e+18  3.0049484e+00  1.2570000e+03\n",
      "   4.7810000e-03 -6.2155000e-02]\n",
      " [ 1.0010000e+03  1.5909696e+18  3.0192919e+00  1.2630000e+03\n",
      "   1.4343500e-02  9.5625000e-03]\n",
      " [ 1.0010000e+03  1.5935616e+18  3.0838373e+00  1.2900000e+03\n",
      "   6.4545400e-02  5.0201900e-02]\n",
      " [ 1.0010000e+03  1.5962400e+18  3.1746790e+00  1.3280000e+03\n",
      "   9.0841700e-02  2.6296300e-02]\n",
      " [ 1.0010000e+03  1.5989184e+18  3.2057564e+00  1.3410000e+03\n",
      "   3.1077400e-02 -5.9764300e-02]\n",
      " [ 1.0010000e+03  1.6015104e+18  3.1938035e+00  1.3360000e+03\n",
      "  -1.1952900e-02 -4.3030300e-02]\n",
      " [ 1.0010000e+03  1.6041888e+18  3.0384164e+00  1.2710000e+03\n",
      "  -1.5538710e-01 -1.4343420e-01]\n",
      " [ 1.0010000e+03  1.6067808e+18  3.0025580e+00  1.2560000e+03\n",
      "  -3.5858400e-02  1.1952870e-01]\n",
      " [ 1.0010000e+03  1.6094592e+18  2.9472437e+00  1.2430000e+03\n",
      "  -5.5314300e-02 -1.9455900e-02]\n",
      " [ 1.0010000e+03  1.6121376e+18  3.1061056e+00  1.3100000e+03\n",
      "   1.5886190e-01  2.1417620e-01]\n",
      " [ 1.0010000e+03  1.6145568e+18  3.1440427e+00  1.3260000e+03\n",
      "   3.7937100e-02 -1.2092480e-01]\n",
      " [ 1.0010000e+03  1.6172352e+18  3.2246592e+00  1.3600000e+03\n",
      "   8.0616500e-02  4.2679400e-02]\n",
      " [ 1.0010000e+03  1.6198272e+18  3.2270303e+00  1.3610000e+03\n",
      "   2.3711000e-03 -7.8245400e-02]\n",
      " [ 1.0010000e+03  1.6225056e+18  3.2222881e+00  1.3590000e+03\n",
      "  -4.7422000e-03 -7.1133000e-03]\n",
      " [ 1.0010000e+03  1.6250976e+18  3.2104328e+00  1.3540000e+03\n",
      "  -1.1855300e-02 -7.1131000e-03]\n",
      " [ 1.0010000e+03  1.6277760e+18  3.2199171e+00  1.3580000e+03\n",
      "   9.4843000e-03  2.1339600e-02]\n",
      " [ 1.0010000e+03  1.6304544e+18  3.1867220e+00  1.3440000e+03\n",
      "  -3.3195100e-02 -4.2679400e-02]\n",
      " [ 1.0010000e+03  1.6330464e+18  3.2033195e+00  1.3510000e+03\n",
      "   1.6597500e-02  4.9792600e-02]\n",
      " [ 1.0010000e+03  1.6357248e+18  3.2009485e+00  1.3500000e+03\n",
      "  -2.3710000e-03 -1.8968500e-02]\n",
      " [ 1.0010000e+03  1.6383168e+18  3.2863071e+00  1.3860000e+03\n",
      "   8.5358600e-02  8.7729600e-02]\n",
      " [ 1.0010000e+03  1.6409952e+18  3.2967808e+00  1.4010000e+03\n",
      "   1.0473700e-02 -7.4884900e-02]\n",
      " [ 1.0010000e+03  1.6436736e+18  3.3344314e+00  1.4170000e+03\n",
      "   3.7650600e-02  2.7176900e-02]\n",
      " [ 1.0010000e+03  1.6460928e+18  3.3367846e+00  1.4180000e+03\n",
      "   2.3532000e-03 -3.5297400e-02]\n",
      " [ 1.0010000e+03  1.6487712e+18  3.3720820e+00  1.4330000e+03\n",
      "   3.5297400e-02  3.2944200e-02]\n",
      " [ 1.0010000e+03  1.6513632e+18  3.3132529e+00  1.4080000e+03\n",
      "  -5.8829100e-02 -9.4126500e-02]\n",
      " [ 1.0010000e+03  1.6540416e+18  3.3461974e+00  1.4220000e+03\n",
      "   3.2944500e-02  9.1773600e-02]\n",
      " [ 1.0010000e+03  1.6566336e+18  3.4379706e+00  1.4610000e+03\n",
      "   9.1773200e-02  5.8828700e-02]\n",
      " [ 1.0010000e+03  1.6593120e+18  3.4238517e+00  1.4550000e+03\n",
      "  -1.4118900e-02 -1.0589210e-01]\n",
      " [ 1.0010000e+03  1.6619904e+18  3.4426770e+00  1.4630000e+03\n",
      "   1.8825300e-02  3.2944200e-02]\n",
      " [ 1.0010000e+03  1.6645824e+18  3.4638555e+00  1.4720000e+03\n",
      "   2.1178500e-02  2.3532000e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Print out some diagnostic info about the last set of timepoints\n",
    "print(f'Timepoints shape: {timepoints.shape}') # type: ignore\n",
    "print()\n",
    "print('Column types:')\n",
    "\n",
    "for column in timepoints[0,0,0,0:]: # type: ignore\n",
    "    print(f'\\t{type(column)}')\n",
    "\n",
    "print()\n",
    "print(f'Example block:\\n{timepoints[0,0,0:,]}') # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks good. We could use int for some of these columns, but we need float for the MBD, so let's leave it alone rather than using mixed types in a NumPy array. So, that's it - easy. Let's round trip it and try recovering some our values back into a nice pandas dataframe as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timepoints shape: (1, 3135, 37, 6)\n",
      "Example block:\n",
      "[[ 1.0010000e+03  1.5698880e+18  3.0558431e+00  1.2690000e+03\n",
      "   1.7097300e-01  2.9378470e-01]\n",
      " [ 1.0010000e+03  1.5725664e+18  2.9932332e+00  1.2430000e+03\n",
      "  -6.2609900e-02 -2.3358290e-01]\n",
      " [ 1.0010000e+03  1.5751584e+18  2.9932332e+00  1.2430000e+03\n",
      "   0.0000000e+00  6.2609900e-02]\n",
      " [ 1.0010000e+03  1.5778368e+18  2.9690900e+00  1.2420000e+03\n",
      "  -2.4143200e-02 -2.4143200e-02]\n",
      " [ 1.0010000e+03  1.5805152e+18  2.9093256e+00  1.2170000e+03\n",
      "  -5.9764400e-02 -3.5621200e-02]\n",
      " [ 1.0010000e+03  1.5830208e+18  2.9332314e+00  1.2270000e+03\n",
      "   2.3905800e-02  8.3670200e-02]\n",
      " [ 1.0010000e+03  1.5856992e+18  3.0001674e+00  1.2550000e+03\n",
      "   6.6936000e-02  4.3030200e-02]\n",
      " [ 1.0010000e+03  1.5882912e+18  3.0049484e+00  1.2570000e+03\n",
      "   4.7810000e-03 -6.2155000e-02]\n",
      " [ 1.0010000e+03  1.5909696e+18  3.0192919e+00  1.2630000e+03\n",
      "   1.4343500e-02  9.5625000e-03]\n",
      " [ 1.0010000e+03  1.5935616e+18  3.0838373e+00  1.2900000e+03\n",
      "   6.4545400e-02  5.0201900e-02]\n",
      " [ 1.0010000e+03  1.5962400e+18  3.1746790e+00  1.3280000e+03\n",
      "   9.0841700e-02  2.6296300e-02]\n",
      " [ 1.0010000e+03  1.5989184e+18  3.2057564e+00  1.3410000e+03\n",
      "   3.1077400e-02 -5.9764300e-02]\n",
      " [ 1.0010000e+03  1.6015104e+18  3.1938035e+00  1.3360000e+03\n",
      "  -1.1952900e-02 -4.3030300e-02]\n",
      " [ 1.0010000e+03  1.6041888e+18  3.0384164e+00  1.2710000e+03\n",
      "  -1.5538710e-01 -1.4343420e-01]\n",
      " [ 1.0010000e+03  1.6067808e+18  3.0025580e+00  1.2560000e+03\n",
      "  -3.5858400e-02  1.1952870e-01]\n",
      " [ 1.0010000e+03  1.6094592e+18  2.9472437e+00  1.2430000e+03\n",
      "  -5.5314300e-02 -1.9455900e-02]\n",
      " [ 1.0010000e+03  1.6121376e+18  3.1061056e+00  1.3100000e+03\n",
      "   1.5886190e-01  2.1417620e-01]\n",
      " [ 1.0010000e+03  1.6145568e+18  3.1440427e+00  1.3260000e+03\n",
      "   3.7937100e-02 -1.2092480e-01]\n",
      " [ 1.0010000e+03  1.6172352e+18  3.2246592e+00  1.3600000e+03\n",
      "   8.0616500e-02  4.2679400e-02]\n",
      " [ 1.0010000e+03  1.6198272e+18  3.2270303e+00  1.3610000e+03\n",
      "   2.3711000e-03 -7.8245400e-02]\n",
      " [ 1.0010000e+03  1.6225056e+18  3.2222881e+00  1.3590000e+03\n",
      "  -4.7422000e-03 -7.1133000e-03]\n",
      " [ 1.0010000e+03  1.6250976e+18  3.2104328e+00  1.3540000e+03\n",
      "  -1.1855300e-02 -7.1131000e-03]\n",
      " [ 1.0010000e+03  1.6277760e+18  3.2199171e+00  1.3580000e+03\n",
      "   9.4843000e-03  2.1339600e-02]\n",
      " [ 1.0010000e+03  1.6304544e+18  3.1867220e+00  1.3440000e+03\n",
      "  -3.3195100e-02 -4.2679400e-02]\n",
      " [ 1.0010000e+03  1.6330464e+18  3.2033195e+00  1.3510000e+03\n",
      "   1.6597500e-02  4.9792600e-02]\n",
      " [ 1.0010000e+03  1.6357248e+18  3.2009485e+00  1.3500000e+03\n",
      "  -2.3710000e-03 -1.8968500e-02]\n",
      " [ 1.0010000e+03  1.6383168e+18  3.2863071e+00  1.3860000e+03\n",
      "   8.5358600e-02  8.7729600e-02]\n",
      " [ 1.0010000e+03  1.6409952e+18  3.2967808e+00  1.4010000e+03\n",
      "   1.0473700e-02 -7.4884900e-02]\n",
      " [ 1.0010000e+03  1.6436736e+18  3.3344314e+00  1.4170000e+03\n",
      "   3.7650600e-02  2.7176900e-02]\n",
      " [ 1.0010000e+03  1.6460928e+18  3.3367846e+00  1.4180000e+03\n",
      "   2.3532000e-03 -3.5297400e-02]\n",
      " [ 1.0010000e+03  1.6487712e+18  3.3720820e+00  1.4330000e+03\n",
      "   3.5297400e-02  3.2944200e-02]\n",
      " [ 1.0010000e+03  1.6513632e+18  3.3132529e+00  1.4080000e+03\n",
      "  -5.8829100e-02 -9.4126500e-02]\n",
      " [ 1.0010000e+03  1.6540416e+18  3.3461974e+00  1.4220000e+03\n",
      "   3.2944500e-02  9.1773600e-02]\n",
      " [ 1.0010000e+03  1.6566336e+18  3.4379706e+00  1.4610000e+03\n",
      "   9.1773200e-02  5.8828700e-02]\n",
      " [ 1.0010000e+03  1.6593120e+18  3.4238517e+00  1.4550000e+03\n",
      "  -1.4118900e-02 -1.0589210e-01]\n",
      " [ 1.0010000e+03  1.6619904e+18  3.4426770e+00  1.4630000e+03\n",
      "   1.8825300e-02  3.2944200e-02]\n",
      " [ 1.0010000e+03  1.6645824e+18  3.4638555e+00  1.4720000e+03\n",
      "   2.1178500e-02  2.3532000e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Check round-trip\n",
    "loaded_timepoints = np.load(output_file)\n",
    "\n",
    "# Inspect\n",
    "print(f'Timepoints shape: {loaded_timepoints.shape}')\n",
    "print(f'Example block:\\n{loaded_timepoints[0,0,0:,]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, not surprisingly - we got the same thing back. Last thing to do before we call this done is to test if we can get our dates, row_ids and cfips back into a format that matches the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dates: [1.5698880e+18 1.5725664e+18 1.5751584e+18 1.5778368e+18 1.5805152e+18\n",
      " 1.5830208e+18 1.5856992e+18 1.5882912e+18 1.5909696e+18 1.5935616e+18\n",
      " 1.5962400e+18 1.5989184e+18 1.6015104e+18 1.6041888e+18 1.6067808e+18\n",
      " 1.6094592e+18 1.6121376e+18 1.6145568e+18 1.6172352e+18 1.6198272e+18\n",
      " 1.6225056e+18 1.6250976e+18 1.6277760e+18 1.6304544e+18 1.6330464e+18\n",
      " 1.6357248e+18 1.6383168e+18 1.6409952e+18 1.6436736e+18 1.6460928e+18\n",
      " 1.6487712e+18 1.6513632e+18 1.6540416e+18 1.6566336e+18 1.6593120e+18\n",
      " 1.6619904e+18 1.6645824e+18]\n",
      "dtype: <class 'numpy.ndarray'>\n",
      "element dtype: <class 'numpy.float64'>\n",
      "\n",
      "Test dates: [1569888000000000000 1572566400000000000 1575158400000000000\n",
      " 1577836800000000000 1580515200000000000 1583020800000000000\n",
      " 1585699200000000000 1588291200000000000 1590969600000000000\n",
      " 1593561600000000000 1596240000000000000 1598918400000000000\n",
      " 1601510400000000000 1604188800000000000 1606780800000000000\n",
      " 1609459200000000000 1612137600000000000 1614556800000000000\n",
      " 1617235200000000000 1619827200000000000 1622505600000000000\n",
      " 1625097600000000000 1627776000000000000 1630454400000000000\n",
      " 1633046400000000000 1635724800000000000 1638316800000000000\n",
      " 1640995200000000000 1643673600000000000 1646092800000000000\n",
      " 1648771200000000000 1651363200000000000 1654041600000000000\n",
      " 1656633600000000000 1659312000000000000 1661990400000000000\n",
      " 1664582400000000000]\n",
      "dtype: <class 'numpy.ndarray'>\n",
      "element dtype: <class 'numpy.int64'>\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37 entries, 0 to 36\n",
      "Data columns (total 1 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   first_day_of_month  37 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1)\n",
      "memory usage: 424.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Grab an example date column\n",
    "test_dates = loaded_timepoints[0,0,0:,1] # type: ignore\n",
    "print(f'Test dates: {test_dates}\\ndtype: {type(test_dates)}\\nelement dtype: {type(test_dates[0])}\\n')\n",
    "\n",
    "# Cast float64 to int64\n",
    "test_dates = test_dates.astype(np.int64)\n",
    "print(f'Test dates: {test_dates}\\ndtype: {type(test_dates)}\\nelement dtype: {type(test_dates[0])}\\n')\n",
    "\n",
    "# Convert to pandas dataframe with dtype datetime64[ns] and column name 'first_day_of_month'\n",
    "test_dates_df = pd.DataFrame(pd.to_datetime(test_dates), columns=['first_day_of_month']).astype('datetime64')\n",
    "test_dates_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_day_of_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_day_of_month\n",
       "0         2019-10-01\n",
       "1         2019-11-01\n",
       "2         2019-12-01\n",
       "3         2020-01-01\n",
       "4         2020-02-01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cfips: [1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001.\n",
      " 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001.\n",
      " 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001. 1001.\n",
      " 1001.]\n",
      "dtype: <class 'numpy.ndarray'>\n",
      "element dtype: <class 'numpy.float64'>\n",
      "\n",
      "Test cfips: [1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001\n",
      " 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001\n",
      " 1001 1001 1001 1001 1001 1001 1001 1001 1001]\n",
      "dtype: <class 'numpy.ndarray'>\n",
      "element dtype: <class 'numpy.int64'>\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37 entries, 0 to 36\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   cfips   37 non-null     int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 424.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Grab an example cfips column\n",
    "test_cfips = loaded_timepoints[0,0,0:,0] # type: ignore\n",
    "print(f'Test cfips: {test_cfips}\\ndtype: {type(test_cfips)}\\nelement dtype: {type(test_cfips[0])}\\n')\n",
    "\n",
    "# Cast float64 to int64\n",
    "test_cfips = test_cfips.astype(np.int64)\n",
    "print(f'Test cfips: {test_cfips}\\ndtype: {type(test_cfips)}\\nelement dtype: {type(test_cfips[0])}\\n')\n",
    "\n",
    "# Convert to pandas dataframe with dtype int64 and column name 'cfips'\n",
    "test_cfips_df = pd.DataFrame(test_cfips, columns=['cfips']).astype('int64')\n",
    "test_cfips_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cfips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cfips\n",
       "0   1001\n",
       "1   1001\n",
       "2   1001\n",
       "3   1001\n",
       "4   1001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cfips_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, happy - making the rwo ID is just a string join from here, so no problem. An if for some reason we want the string county or state back, we can use a CFIPS lookup table. Time to move on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89e0f329143aafc2740b6540b46c06e92791a1e818eb6a9ece1d952786ba476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
