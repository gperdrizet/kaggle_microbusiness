{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 5: Structured bootstrapping\n",
    "OK, so internal benchmarking strategy #2 here. Going to start calling it bootstrapping because I am using random sampling with replacement and am envisioning a bagging approach to the final model. Big takeaway from the first cross validation-like approach (really ended up being more like bootstrapping anyway...) was that to get good approximations of the public leader board score we should not be aggregating SMAPE scores across forecast origins. This seems to result in worse performance, the explanation being that we are including data from some anomalous and some nonanomalous timepoints. When, in reality a given timepoint is either an anomaly or not. This causes the naive model to do somewhat badly all the time rather than OK on nonanomalous test time points and very badly on anomalous ones.\n",
    "\n",
    "So, the strategy here will be to structure our bootstrapping while still treating each county as an individual datapoint. To generate a sample of size n the procedure will be as follows:\n",
    "1. Remove and sequester some timepoints dataset-wide for test data.\n",
    "2. From the remaining, pick a random timepoint.\n",
    "3. From the randomly chosen timepoint randomly choose n counties.\n",
    "4. Go to step 2.\n",
    "\n",
    "This procedure could be repeated with different test-train splits, or even test train splits on non-overlapping subsets of the data. Does that make it bootstrapping inside of cross-validation fold? I don't know - I think the specific terminology here is less important than a precise description of what we are actually doing.\n",
    "\n",
    "\n",
    "1. [Abbreviations & definitions](#abbrevations_definitions)\n",
    "2. [Load & inspect](#load_inspect)\n",
    "3. [Build data structure](#build_data_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"abbreviations_definitions\"></a>\n",
    "### 1. Abbreviations & definitions\n",
    "+ MBD: microbusiness density\n",
    "+ MBC: microbusiness count\n",
    "+ OLS: ordinary least squares\n",
    "+ Model order: number of past timepoints used as input data for model training\n",
    "+ Origin (forecast origin): last known point in the input data\n",
    "+ Horizon (forecast horizon): number of future data points predicted by the model\n",
    "+ SMAPE: Symmetric mean absolute percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_inspect\"></a>\n",
    "### 2. Load & inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:24:10) [GCC 9.4.0]\n",
      "\n",
      "Numpy 1.23.5\n",
      "Pandas 1.4.3\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path to allow import of config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import config as conf\n",
    "import functions.data_manipulation_functions as data_funcs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from statistics import NormalDist\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print()\n",
    "print(f'Numpy {np.__version__}')\n",
    "print(f'Pandas {pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>cfips</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>microbusiness_density</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001_2019-08-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001_2019-09-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2.884870</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001_2019-10-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>3.055843</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001_2019-11-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001_2019-12-01</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  cfips          county    state first_day_of_month  \\\n",
       "0  1001_2019-08-01   1001  Autauga County  Alabama         2019-08-01   \n",
       "1  1001_2019-09-01   1001  Autauga County  Alabama         2019-09-01   \n",
       "2  1001_2019-10-01   1001  Autauga County  Alabama         2019-10-01   \n",
       "3  1001_2019-11-01   1001  Autauga County  Alabama         2019-11-01   \n",
       "4  1001_2019-12-01   1001  Autauga County  Alabama         2019-12-01   \n",
       "\n",
       "   microbusiness_density  active  \n",
       "0               3.007682    1249  \n",
       "1               2.884870    1198  \n",
       "2               3.055843    1269  \n",
       "3               2.993233    1243  \n",
       "4               2.993233    1243  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up training file, set dtype for first day of month\n",
    "training_df = pd.read_csv(f'{conf.KAGGLE_DATA_PATH}/train.csv.zip', compression='zip')\n",
    "training_df['first_day_of_month'] =  pd.to_datetime(training_df['first_day_of_month'])\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"build_data_structure\"></a>\n",
    "### 1. Build data structure\n",
    "First thing to do is build a data structure that will make our structured sampling approach easy and persist it to disk. Then we will work on our sampling strategy. This way when experimenting models, it will be easy to generate samples on they fly without having to rebuild the whole thing in memory each time.\n",
    "\n",
    "The trick here is that in the first dimension we want the timepoints, then the counties, then the data columns. But, the 'timepoints' we want in the first dimension are really the forecast origin for a subset of the timecourse.\n",
    "\n",
    "Maybe the best way to think about it is to forget the model order, forecast horizon and forecast origin location for now and just pick a data instance size. Then, when feeding the sample to a model we can break each block into input and forecast halves on the fly however we want. This will also solve the problem we had in notebook #07 where if model order != forecast horizon NumPy complains about ragged arrays.\n",
    "\n",
    "Not sure if there maybe would be a speed advantage of sharding the data to multiple files here - i.e. each timepoint gets its own file and then worker processes can be assigned to sample from and train on the timepoints independently. Let's save that for a later optimization to minimize complexity out of the gate and not spend time prematurely optimizing something that ends up not being the right idea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microbusiness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89e0f329143aafc2740b6540b46c06e92791a1e818eb6a9ece1d952786ba476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
